{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eb8ec0b",
   "metadata": {},
   "source": [
    "# 1. Background of the Study: Multi-Label Product Classifier\n",
    "This notebook documents the development of a Multi-Label Classification Model for the Lumora e-commerce platform.\n",
    "\n",
    "---\n",
    "\n",
    "# Title of the Study\n",
    "## **Lumora: Multi-Label NLP Classifier for Automatic Tagging of Filipino Contemporary Arts and Crafts**\n",
    "\n",
    "---\n",
    "\n",
    "# Source of Data\n",
    "The dataset, named LumoraProductDataset.csv, was collected from various online sources showcasing Filipino handmade goods and artisanal products.\n",
    "\n",
    "- Source: Aggregated data from various public e-commerce listings focusing on Filipino handcrafted goods.\n",
    "- Original Format: CSV/Excel tabular data.\n",
    "\n",
    "---\n",
    "\n",
    "# Brief Description of Dataset\n",
    "This dataset consists of unique product listings designed to train an automated tagging model for the Lumora C2C platform. Each row represents a single handcrafted or creative item.\n",
    "\n",
    "- Data Dimensions: The initial dataset contained 644 rows and 12 columns. \n",
    "\n",
    "- Meaning of Each Variable (Selected for Modeling):\n",
    "\n",
    "1. `Product name` and `Product description`: Primary text fields used to infer the tags.\n",
    "\n",
    "2. `Color`, `Size`, `Material`: Secondary text fields concatenated to enrich the product context.\n",
    "\n",
    "3. `Tags`: The comma-separated field of labels manually assigned to the product (e.g., wedding, tote, floral embroidery, Filipino, sustainable).\n",
    "\n",
    "---\n",
    "\n",
    "# Model Variables\n",
    "| Variable | Description | Role in Model |\n",
    "|--------|-------------|-------------|\n",
    "| **Selected Features (Independent Variables)** | The concatenated and pre-processed text derived from the Product name, Product description, Color, Size, and Material columns. | **X (Input)**: This single text input is converted into a numerical vector (e.g., using TF-IDF). |\n",
    "| **Target / Label Column (Dependent Variable)** | The cleaned Tags column. This column will be converted into a binary matrix where each unique tag (e.g., cute, crochet, minimalist) is a separate binary feature (0 or 1). | **Y (Output)**: The labels the model is trained to predict simultaneously for a given product. |\n",
    "\n",
    "---\n",
    "\n",
    "# Objective\n",
    "The objective is to develop a highly accurate Multi-Label NLP Classifier that can automatically assign a set of relevant categories and stylistic attributes to new product listings. This model will reduce the manual effort for sellers and ensure new products are appropriately tagged (e.g., keychain, crochet, kawaii, minimalist), thereby improving product discoverability on the Lumora platform for both general and niche search queries.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16414b3",
   "metadata": {},
   "source": [
    "# 2. Data Collection / Loading\n",
    "The data used for training the Multi-Label Classifier model is sourced from the `LumoraProductDataset.csv` file, which aggregates product listings from various Filipino arts and crafts sellers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "956c2ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product name</th>\n",
       "      <th>Product description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Category</th>\n",
       "      <th>Subcategory</th>\n",
       "      <th>Color</th>\n",
       "      <th>Size</th>\n",
       "      <th>Material</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Product link</th>\n",
       "      <th>Image link</th>\n",
       "      <th>Brand / seller name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Flowers Convertible Puso Wedding Tote</td>\n",
       "      <td>A versatile hobo-style tote embroidered with f...</td>\n",
       "      <td>PHP 11172.22</td>\n",
       "      <td>Bags</td>\n",
       "      <td>Wedding Tote</td>\n",
       "      <td>White</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Upcycled fabric, leather</td>\n",
       "      <td>wedding, tote, floral embroidery, Filipino, su...</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>SintaWeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Manila Jeepney 3-in-1 Handbag</td>\n",
       "      <td>A colorful handbag inspired by the iconic jeep...</td>\n",
       "      <td>PHP 12406.79</td>\n",
       "      <td>Bags</td>\n",
       "      <td>Handbag</td>\n",
       "      <td>Multicolor</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Upcycled fabric, leather</td>\n",
       "      <td>jeepney, handbag, Filipino, sustainable</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>SintaWeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vinia Hardin Fanny Pack</td>\n",
       "      <td>A belt-style fanny pack handwoven with upcycle...</td>\n",
       "      <td>PHP 4875.93</td>\n",
       "      <td>Bags</td>\n",
       "      <td>Fanny Pack</td>\n",
       "      <td>Black</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Upcycled fabric, leather</td>\n",
       "      <td>fanny pack, Filipino, sustainable</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>SintaWeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sling Bag (Pinilian/Inabel Weave)</td>\n",
       "      <td>A crossbody sling bag showcasing traditional P...</td>\n",
       "      <td>PHP 5554.94</td>\n",
       "      <td>Bags</td>\n",
       "      <td>Sling Bag</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Upcycled fabric, Pinilian/Inabel weave</td>\n",
       "      <td>sling bag, Filipino, handwoven, sustainable</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>SintaWeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alon Woven Waves Shoulder Bag</td>\n",
       "      <td>A shoulder bag with wave-pattern weaving, comb...</td>\n",
       "      <td>PHP 12653.70</td>\n",
       "      <td>Bags</td>\n",
       "      <td>Shoulder Bag</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Upcycled fabric, leather</td>\n",
       "      <td>shoulder bag, woven waves, Filipino, sustainable</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>SintaWeddings</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Product name  \\\n",
       "0  Flowers Convertible Puso Wedding Tote   \n",
       "1          Manila Jeepney 3-in-1 Handbag   \n",
       "2                Vinia Hardin Fanny Pack   \n",
       "3      Sling Bag (Pinilian/Inabel Weave)   \n",
       "4          Alon Woven Waves Shoulder Bag   \n",
       "\n",
       "                                 Product description         Price Category  \\\n",
       "0  A versatile hobo-style tote embroidered with f...  PHP 11172.22     Bags   \n",
       "1  A colorful handbag inspired by the iconic jeep...  PHP 12406.79     Bags   \n",
       "2  A belt-style fanny pack handwoven with upcycle...   PHP 4875.93     Bags   \n",
       "3  A crossbody sling bag showcasing traditional P...   PHP 5554.94     Bags   \n",
       "4  A shoulder bag with wave-pattern weaving, comb...  PHP 12653.70     Bags   \n",
       "\n",
       "    Subcategory       Color         Size  \\\n",
       "0  Wedding Tote       White  Unspecified   \n",
       "1       Handbag  Multicolor  Unspecified   \n",
       "2    Fanny Pack       Black  Unspecified   \n",
       "3     Sling Bag        Blue  Unspecified   \n",
       "4  Shoulder Bag        Blue  Unspecified   \n",
       "\n",
       "                                 Material  \\\n",
       "0                Upcycled fabric, leather   \n",
       "1                Upcycled fabric, leather   \n",
       "2                Upcycled fabric, leather   \n",
       "3  Upcycled fabric, Pinilian/Inabel weave   \n",
       "4                Upcycled fabric, leather   \n",
       "\n",
       "                                                Tags Product link  \\\n",
       "0  wedding, tote, floral embroidery, Filipino, su...  Unspecified   \n",
       "1            jeepney, handbag, Filipino, sustainable  Unspecified   \n",
       "2                  fanny pack, Filipino, sustainable  Unspecified   \n",
       "3        sling bag, Filipino, handwoven, sustainable  Unspecified   \n",
       "4   shoulder bag, woven waves, Filipino, sustainable  Unspecified   \n",
       "\n",
       "    Image link Brand / seller name  \n",
       "0  Unspecified       SintaWeddings  \n",
       "1  Unspecified       SintaWeddings  \n",
       "2  Unspecified       SintaWeddings  \n",
       "3  Unspecified       SintaWeddings  \n",
       "4  Unspecified       SintaWeddings  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "# initializing dataframe\n",
    "df = pd.read_csv('Lumora_Product_Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe10bd7",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Data Information and Summary Statistics\n",
    "This section presents the initial inspection of the loaded dataset to understand its structure, completeness, and the distribution of the key variables.\n",
    "\n",
    "## Initial Data Inspection\n",
    "The initial inspection confirms the overall data integrity, type, and dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb8c54db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 644 entries, 0 to 643\n",
      "Data columns (total 12 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   Product name         644 non-null    object\n",
      " 1   Product description  644 non-null    object\n",
      " 2   Price                644 non-null    object\n",
      " 3   Category             644 non-null    object\n",
      " 4   Subcategory          644 non-null    object\n",
      " 5   Color                644 non-null    object\n",
      " 6   Size                 644 non-null    object\n",
      " 7   Material             644 non-null    object\n",
      " 8   Tags                 644 non-null    object\n",
      " 9   Product link         644 non-null    object\n",
      " 10  Image link           644 non-null    object\n",
      " 11  Brand / seller name  639 non-null    object\n",
      "dtypes: object(12)\n",
      "memory usage: 60.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# Show column types and non-null counts\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba815565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(644, 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show dataset dimensions\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a47b130c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Product name           0\n",
       "Product description    0\n",
       "Price                  0\n",
       "Category               0\n",
       "Subcategory            0\n",
       "Color                  0\n",
       "Size                   0\n",
       "Material               0\n",
       "Tags                   0\n",
       "Product link           0\n",
       "Image link             0\n",
       "Brand / seller name    5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show count of missing values per column\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e076fd6",
   "metadata": {},
   "source": [
    "## Summary of Initial Findings\n",
    "- **Data Dimensions**: The initial raw dataset contains 644 entries (rows) and 12 columns.\n",
    "\n",
    "- **Missing Values**: Only the Brand / seller name column has 5 missing (non-null) values. Since this column is not directly used for the NLP model's input text content or the target tags, these missing values will be handled by dropping the column during the cleaning phase.\n",
    "\n",
    "- **Data Types**: All columns are of the generic object (string) type, which is expected since the majority of the columns (Product name, Product description, Tags, Material, etc.) are text-based inputs for the NLP model.\n",
    "\n",
    "# Key Metric Analysis (Categorical and Target)\n",
    "Since this is a classification problem, it is crucial to analyze the unique values and frequency distribution of the target column (Tags) and other categorical columns that influence it (Category)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd056629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Category Distribution:\n",
      "\n",
      "                            count\n",
      "Category                         \n",
      "Jewelry                       151\n",
      "Vintage                        59\n",
      "Ornaments                      46\n",
      "POD                            28\n",
      "Stickers                       25\n",
      "Clothing                       23\n",
      "Philippine Handicrafts         22\n",
      "Digital Downloads              22\n",
      "Stickers/Decals                19\n",
      "Philippine Souvenir            18\n",
      "Pasko & Parols                 17\n",
      "Keychains/Charms               17\n",
      "Wedding Ceremony               16\n",
      "Bags                           16\n",
      "Accessories                    16\n",
      "Bundle Deals                   16\n",
      "Filipiniana Attire             13\n",
      "Apparel                        13\n",
      "Prints                         12\n",
      "Pinoy Keychains & Charms       10\n",
      "Vintage Movies                  9\n",
      "Stationery & Stickers           9\n",
      "Keychains                       8\n",
      "Native                          7\n",
      "Capiz Decor                     6\n",
      "Printables                      5\n",
      "Mugs                            4\n",
      "Decor                           4\n",
      "Art Prints                      3\n",
      "Car Accessories                 3\n",
      "Baybayin                        3\n",
      "Pins                            3\n",
      "Motorcycle Accessories          3\n",
      "Home Goods                      3\n",
      "Jewelry & Pouches               2\n",
      "Christmas Ornaments             2\n",
      "Pochette                        2\n",
      "Keychains/Hair Accessories      1\n",
      "Magnets                         1\n",
      "Hair Accessories                1\n",
      "Framed Prints                   1\n",
      "Face Bag                        1\n",
      "Greeting Cards                  1\n",
      "Objects                         1\n",
      "Reusable Bags                   1\n",
      "Bracelets                       1\n"
     ]
    }
   ],
   "source": [
    "# Show descriptive statistics for object columns\n",
    "df.describe(include='object')\n",
    "\n",
    "# Show value counts for the primary Category column\n",
    "category_df = df['Category'].value_counts().to_frame()\n",
    "print(\"\\nCategory Distribution:\\n\")\n",
    "print(category_df.head(46))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6425783",
   "metadata": {},
   "source": [
    "## Summary of Key Metrics\n",
    "**Categories**: There are 46 unique categories in the dataset. The most frequent category is \"Jewelry\" (151 counts), followed by \"Vintage\" (59 counts). This unequal distribution is common and must be considered during modeling.\n",
    "\n",
    "**Product Names/Descriptions**: There are 601 unique product names and 560 unique product descriptions out of 644 total entries, suggesting high diversity among the listed products.\n",
    "\n",
    "**Tags (Target Variable)**: The Tags column has 565 unique values out of 644 total rows. This high cardinality confirms that the problem is highly suited for Multi-Label Classification, as most products are uniquely tagged with a combination of attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a840ea2b",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Data Cleaning\n",
    "This section details the critical data cleaning operations performed on the raw text dataset to ensure consistency, handle missing values, and prepare the data for subsequent feature engineering.\n",
    "\n",
    "## A. Handle Missing Values\n",
    "Missing values (NaN or empty strings) in text columns can disrupt the NLP pipeline. Based on the initial data inspection, only the Brand / seller name column had 5 missing values. Since this column is text-based and its content might be useful for enrichment, we fill the missing values with an empty string ('') rather than dropping the entire row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a85159fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after filling:\n",
      "Product name           0\n",
      "Product description    0\n",
      "Price                  0\n",
      "Category               0\n",
      "Subcategory            0\n",
      "Color                  0\n",
      "Size                   0\n",
      "Material               0\n",
      "Tags                   0\n",
      "Product link           0\n",
      "Image link             0\n",
      "Brand / seller name    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values by filling NaN with an empty string (for text compatibility)\n",
    "df = df.fillna('')\n",
    "\n",
    "# Verify that all missing values have been handled\n",
    "print(\"Missing values after filling:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121addcc",
   "metadata": {},
   "source": [
    "## B. Handle Duplicate Rows\n",
    "Duplicate product listings can skew frequency analysis and unnecessarily increase model training time. We remove any identical rows that may have resulted from data scraping or entry errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9601f098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial rows: 644\n",
      "Rows after dropping duplicates: 618\n",
      "Total duplicates removed: 26\n",
      "New DataFrame shape: (618, 12)\n"
     ]
    }
   ],
   "source": [
    "# Drop any completely duplicate rows and update the DataFrame in place\n",
    "initial_rows = df.shape[0]\n",
    "df = df.drop_duplicates()\n",
    "rows_after_cleaning = df.shape[0]\n",
    "\n",
    "# Report the change in dimensions\n",
    "print(f\"\\nInitial rows: {initial_rows}\")\n",
    "print(f\"Rows after dropping duplicates: {rows_after_cleaning}\")\n",
    "print(f\"Total duplicates removed: {initial_rows - rows_after_cleaning}\")\n",
    "print(f\"New DataFrame shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932d7a3d",
   "metadata": {},
   "source": [
    "## C. Standardize Inconsistent Data (Text Normalization)\n",
    "Inconsistent text data, such as differing capitalization, irregular spacing, and varied units, confuses the model by treating the same concept (e.g., 'RING' vs. 'ring') as two different entities. We apply normalization steps to the text columns that will serve as model features.\n",
    "\n",
    "### 1. Standardize Whitespace and Casing\n",
    "We remove leading/trailing whitespace and reduce multiple spaces between words to a single space. We then convert the feature columns to a standardized casing (e.g., Title Case for categorical fields, Lowercase for the main text fields like Product Description) to group similar terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "642e6909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Whitespace and Casing Standardization applied to categorical features.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product name</th>\n",
       "      <th>Product description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Category</th>\n",
       "      <th>Subcategory</th>\n",
       "      <th>Color</th>\n",
       "      <th>Size</th>\n",
       "      <th>Material</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Product link</th>\n",
       "      <th>Image link</th>\n",
       "      <th>Brand / seller name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Flowers Convertible Puso Wedding Tote</td>\n",
       "      <td>A versatile hobo-style tote embroidered with f...</td>\n",
       "      <td>PHP 11172.22</td>\n",
       "      <td>Bags</td>\n",
       "      <td>Wedding Tote</td>\n",
       "      <td>White</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Upcycled fabric, leather</td>\n",
       "      <td>wedding, tote, floral embroidery, Filipino, su...</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>SintaWeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Manila Jeepney 3-in-1 Handbag</td>\n",
       "      <td>A colorful handbag inspired by the iconic jeep...</td>\n",
       "      <td>PHP 12406.79</td>\n",
       "      <td>Bags</td>\n",
       "      <td>Handbag</td>\n",
       "      <td>Multicolor</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Upcycled fabric, leather</td>\n",
       "      <td>jeepney, handbag, Filipino, sustainable</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>SintaWeddings</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Product name  \\\n",
       "0  Flowers Convertible Puso Wedding Tote   \n",
       "1          Manila Jeepney 3-in-1 Handbag   \n",
       "\n",
       "                                 Product description         Price Category  \\\n",
       "0  A versatile hobo-style tote embroidered with f...  PHP 11172.22     Bags   \n",
       "1  A colorful handbag inspired by the iconic jeep...  PHP 12406.79     Bags   \n",
       "\n",
       "    Subcategory       Color         Size                  Material  \\\n",
       "0  Wedding Tote       White  Unspecified  Upcycled fabric, leather   \n",
       "1       Handbag  Multicolor  Unspecified  Upcycled fabric, leather   \n",
       "\n",
       "                                                Tags Product link  \\\n",
       "0  wedding, tote, floral embroidery, Filipino, su...  Unspecified   \n",
       "1            jeepney, handbag, Filipino, sustainable  Unspecified   \n",
       "\n",
       "    Image link Brand / seller name  \n",
       "0  Unspecified       SintaWeddings  \n",
       "1  Unspecified       SintaWeddings  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of text columns for general cleaning\n",
    "text_columns = ['Product name', 'Product description', 'Category', 'Subcategory', 'Size', 'Material', 'Tags']\n",
    "df_clean = df.copy()\n",
    "\n",
    "# 1. Strip whitespace and reduce multi-spaces\n",
    "for col in text_columns:\n",
    "    df_clean[col] = df_clean[col].str.strip()\n",
    "    df_clean[col] = df_clean[col].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "# 2. Standardize Casing (Note: Categorical data is often left Title/Upper until feature encoding)\n",
    "df_clean['Size'] = df_clean['Size'].str.upper()\n",
    "df_clean['Material'] = df_clean['Material'].str.title()\n",
    "# The main text fields for the model will be lowercased later in the pre-processing stage\n",
    "\n",
    "print(\"\\n✓ Whitespace and Casing Standardization applied to categorical features.\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347ae855",
   "metadata": {},
   "source": [
    "### 2. Standardize Inconsistent Formatting\n",
    "We use regular expressions to fix common inconsistencies in the unstructured data, such as measurement units and common compound words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3caffed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Size and Material formats standardized.\n"
     ]
    }
   ],
   "source": [
    "# 3. Standardize SIZE field inconsistencies (e.g., 'inches' to 'in')\n",
    "def standardize_size(size_str):\n",
    "    if not isinstance(size_str, str) or size_str == '':\n",
    "        return ''\n",
    "    # Standardize \"inches\" variations\n",
    "    size_str = re.sub(r'\\binches\\b', 'in', size_str, flags=re.IGNORECASE)\n",
    "    size_str = re.sub(r'\\binch\\b', 'in', size_str, flags=re.IGNORECASE)\n",
    "    # Standardize 'x' separator\n",
    "    size_str = re.sub(r'\\s*x\\s*', ' x ', size_str, flags=re.IGNORECASE)\n",
    "    # Standardize common abbreviations\n",
    "    size_str = re.sub(r'\\bapprox\\.?\\b', 'Approx.', size_str, flags=re.IGNORECASE)\n",
    "    return size_str.strip()\n",
    "\n",
    "df_clean['Size'] = df_clean['Size'].apply(standardize_size)\n",
    "\n",
    "# 4. Standardize MATERIAL field inconsistencies\n",
    "def standardize_materials(material_str):\n",
    "    if not isinstance(material_str, str) or material_str == '':\n",
    "        return ''\n",
    "    # Standardize common material combinations\n",
    "    material_str = re.sub(r'\\bvinyl sticker with matte finish\\b', 'Vinyl Sticker (Matte Finish)', material_str, flags=re.IGNORECASE)\n",
    "    material_str = re.sub(r'\\bmetal keychain ring\\b', 'Metal Findings', material_str, flags=re.IGNORECASE)\n",
    "    return material_str.title()\n",
    "\n",
    "df_clean['Material'] = df_clean['Material'].apply(standardize_materials)\n",
    "\n",
    "print(\"✓ Size and Material formats standardized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508a8ca6",
   "metadata": {},
   "source": [
    "### 3. Clean Special Characters\n",
    "We clean up stray characters, quotes, and encoding issues that can split words or introduce noise into the tokenization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "795ef263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Special characters cleaned.\n"
     ]
    }
   ],
   "source": [
    "# 5. Fix special characters and encoding issues\n",
    "def clean_special_chars(text):\n",
    "    if not isinstance(text, str) or text == '':\n",
    "        return ''\n",
    "    # Normalize dashes and remove invisible characters\n",
    "    text = text.replace('—', '-').replace('–', '-')\n",
    "    text = re.sub(r'[\\u200b-\\u200f\\u202a-\\u202e\\ufeff]', '', text)\n",
    "    # Remove problematic quotes/symbols (already handled in cleaning step 1)\n",
    "    return text\n",
    "\n",
    "for col in text_columns:\n",
    "    df_clean[col] = df_clean[col].apply(clean_special_chars)\n",
    "\n",
    "print(\"✓ Special characters cleaned.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ecedc2",
   "metadata": {},
   "source": [
    "## Summary of Data Cleaning Operations\n",
    "The data cleaning phase achieved the following:\n",
    "\n",
    "- **Completeness**: All 5 missing values in the Brand / seller name column were successfully filled with empty strings.\n",
    "\n",
    "- **Validity**: 26 duplicate rows were removed, resulting in a cleaner dataset of 618 unique entries for modeling.\n",
    "\n",
    "- **Consistency**: All text-based feature columns (Product name, Description, Size, Material, etc.) were normalized for casing, whitespace, and key format variations, ensuring the NLP model trains on unified concepts (e.g., '2 IN' instead of '2 inches', 'Vinyl Sticker (Matte Finish)' instead of 'vinyl sticker with matte finish')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a7569d",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Data Engineering / Pre-processing\n",
    "The goal of the Multi-Label Classifier is to predict the tags from the product description and related attributes. To provide the model with the richest context, combinine the most descriptive text fields: `Product name`, `Product description`, `Color`, `Size`, and `Material`.\n",
    "\n",
    "## A. Dropping Unnecessary Columns\n",
    "The columns Price, Product link, Image link, and Brand / seller name are unnecessary for the Multi-Label Classifier because they don't help determine the product's descriptive tags, or they introduce noise. \n",
    "1. Irrelevance: The Price is a numerical variable and is generally not a direct semantic feature that dictates the style or material tags of an item.\n",
    "\n",
    "2. Noise: Product link and Image link mostly contain non-semantic URLs that, even after cleaning, would add unnecessary noise to the model's vocabulary.\n",
    "\n",
    "3. Low Value: The Brand / seller name might add a little value, but it's not core to describing the product itself and can introduce bias or overfitting based on a specific seller. Since the column also had missing values that were just filled with empty strings, it's best to exclude it.\n",
    "\n",
    "4. Efficiency: Dropping these columns makes the DataFrame smaller and faster to process during the subsequent NLP steps (Tokenization, Vectorization, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64eae9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Unnecessary columns successfully dropped.\n",
      "New DataFrame shape: (618, 8)\n",
      "Remaining columns: ['Product name', 'Product description', 'Category', 'Subcategory', 'Color', 'Size', 'Material', 'Tags']\n"
     ]
    }
   ],
   "source": [
    "# List of columns to drop as they are not needed for tag prediction\n",
    "columns_to_drop = [\n",
    "    'Price', \n",
    "    'Product link', \n",
    "    'Image link', \n",
    "    'Brand / seller name'\n",
    "]\n",
    "\n",
    "# Drop the columns from the cleaned DataFrame\n",
    "df_clean = df_clean.drop(columns=columns_to_drop, axis=1)\n",
    "\n",
    "print(\"✓ Unnecessary columns successfully dropped.\")\n",
    "print(f\"New DataFrame shape: {df_clean.shape}\")\n",
    "print(f\"Remaining columns: {df_clean.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc705888",
   "metadata": {},
   "source": [
    "## B. Concatination\n",
    "Combining these features into a single string ensures that the model learns the relationship between attributes (e.g., the word \"Blue\" in the Color column) and the resulting tags (e.g., a tag like \"pastel\" or \"ocean-themed\" which might be implied by the description).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15fccfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ TEXT_CONTENT column created successfully.\n",
      "\n",
      "Verification of Final Features and Target:\n",
      "                            Product name                                                                                                              Product description                                                     Tags                                                                                                                                                                                                      TEXT_CONTENT\n",
      "0  Flowers Convertible Puso Wedding Tote  A versatile hobo-style tote embroidered with floral motifs, designed for weddings and crafted from upcycled fabric and leather.  wedding, tote, floral embroidery, Filipino, sustainable  Flowers Convertible Puso Wedding Tote A versatile hobo-style tote embroidered with floral motifs, designed for weddings and crafted from upcycled fabric and leather. White UNSPECIFIED Upcycled Fabric, Leather\n",
      "1          Manila Jeepney 3-in-1 Handbag            A colorful handbag inspired by the iconic jeepney, featuring upcycled fabric and leather with multifunctional design.                  jeepney, handbag, Filipino, sustainable               Manila Jeepney 3-in-1 Handbag A colorful handbag inspired by the iconic jeepney, featuring upcycled fabric and leather with multifunctional design. Multicolor UNSPECIFIED Upcycled Fabric, Leather\n"
     ]
    }
   ],
   "source": [
    "# --- RUN THIS CODE AFTER DROPPING COLUMNS ---\n",
    "\n",
    "# Concatenate relevant text columns into one feature column (TEXT_CONTENT)\n",
    "# We fill any remaining blanks with an empty string and ensure they are strings before concatenation\n",
    "df_clean['TEXT_CONTENT'] = (\n",
    "    df_clean['Product name'].fillna('').astype(str) + ' ' +\n",
    "    df_clean['Product description'].fillna('').astype(str) + ' ' +\n",
    "    df_clean['Color'].fillna('').astype(str) + ' ' +\n",
    "    df_clean['Size'].fillna('').astype(str) + ' ' +\n",
    "    df_clean['Material'].fillna('').astype(str)\n",
    ")\n",
    "\n",
    "print(\"\\n✓ TEXT_CONTENT column created successfully.\")\n",
    "\n",
    "# Display the remaining columns to verify\n",
    "print(\"\\nVerification of Final Features and Target:\")\n",
    "print(df_clean[['Product name', 'Product description', 'Tags', 'TEXT_CONTENT']].head(2).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d6cff3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82fad12b",
   "metadata": {},
   "source": [
    "## C. Tokenization\n",
    "Tokenization is the process of splitting a continuous sequence of text into smaller, meaningful units called tokens. These tokens are typically individual words, but they can also be phrases, numbers, or punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "897b878d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\PLPASIG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenization completed. 'TOKENS' column created.\n",
      "New DataFrame shape: (618, 10)\n",
      "\n",
      "Verification of Tokenization (First Product):\n",
      "--------------------------------------------------\n",
      "Original Text (Snippet): Flowers Convertible Puso Wedding Tote A versatile hobo-style tote embroidered with floral motifs, de...\n",
      "Tokens: ['Flowers', 'Convertible', 'Puso', 'Wedding', 'Tote', 'A', 'versatile', 'hobo-style', 'tote', 'embroidered', 'with', 'floral', 'motifs', ',', 'designed', 'for', 'weddings', 'and', 'crafted', 'from'] (first 20 tokens)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Assuming df_clean is your current DataFrame with the 'TEXT_CONTENT' column\n",
    "\n",
    "# 1. Ensure NLTK resources are available (needed if the kernel was restarted)\n",
    "# nltk.download('punkt', quiet=True) \n",
    "# Note: You only need to run the download once.\n",
    "\n",
    "def tokenize_content(text):\n",
    "    \"\"\"Tokenizes text into a list of individual words.\"\"\"\n",
    "    if not isinstance(text, str) or text == '':\n",
    "        return []\n",
    "    \n",
    "    # Tokenize the text using NLTK's word tokenizer\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply tokenization to the TEXT_CONTENT column\n",
    "df_clean['TOKENS'] = df_clean['TEXT_CONTENT'].apply(tokenize_content)\n",
    "\n",
    "print(\"✓ Tokenization completed. 'TOKENS' column created.\")\n",
    "print(f\"New DataFrame shape: {df_clean.shape}\")\n",
    "\n",
    "# Display verification of the tokens for the first product\n",
    "print(\"\\nVerification of Tokenization (First Product):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Original Text (Snippet): {df_clean['TEXT_CONTENT'].iloc[0][:100]}...\")\n",
    "print(f\"Tokens: {df_clean['TOKENS'].iloc[0][:20]} (first 20 tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad89783",
   "metadata": {},
   "source": [
    "## D. Stopword Removal\n",
    "**Stopword removal** is the process of eliminating common words that appear frequently in text but hold little semantic value or unique meaning for the task, such as `\"a,\" \"the,\" \"is,\" \"and,\" and \"with\"`. Removing these words helps reduce the dimensionality of the text data and focuses the model on the most descriptive keywords (like `crochet`, `kawaii`, `keychain`) that are critical for predicting the product's tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ff024c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Total unique words in stoplist: 221\n",
      "✓ Stopword removal completed. 'TOKENS_FILTERED' column created.\n",
      "New DataFrame shape: (618, 11)\n",
      "\n",
      "Verification of Stopword Removal (First Product):\n",
      "--------------------------------------------------\n",
      "Original Tokens (Snippet): ['Flowers', 'Convertible', 'Puso', 'Wedding', 'Tote', 'A', 'versatile', 'hobo-style', 'tote', 'embroidered', 'with', 'floral', 'motifs', ',', 'designed']\n",
      "Filtered Tokens (Snippet): ['Flowers', 'Convertible', 'Puso', 'Wedding', 'Tote', 'hobo-style', 'embroidered', 'floral', 'motifs', 'weddings', 'crafted', 'upcycled', 'fabric', 'leather', 'White', 'UNSPECIFIED', 'Upcycled', 'Fabric', 'Leather']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PLPASIG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# --- Stopword Setup ---\n",
    "# 1. Get standard English stopwords\n",
    "standard_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# 2. Define custom domain-specific stopwords (e.g., words common to all e-commerce items)\n",
    "custom_stopwords = {\n",
    "    'product', 'item', 'featuring', 'made', 'designed', 'inspired',\n",
    "    'perfect', 'ideal', 'great', 'comes', 'includes', 'set', \n",
    "    'inch', 'pinoy', 'tagalog', 'in', 'approx', 'tote', \n",
    "    'link', 'php', 'style', 'versatile', 'convertible', 'hobo' # Based on observed dataset values\n",
    "}\n",
    "stop_words = standard_stopwords.union(custom_stopwords)\n",
    "print(f\"✓ Total unique words in stoplist: {len(stop_words)}\")\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Removes stopwords and single-character tokens from a list of words.\"\"\"\n",
    "    if not tokens:\n",
    "        return []\n",
    "    \n",
    "    # Filter out stopwords, punctuation, and single characters (e.g., 'A', 'I', 'S')\n",
    "    filtered_tokens = [\n",
    "        token for token in tokens\n",
    "        if token not in stop_words \n",
    "        and token not in string.punctuation\n",
    "        and len(token) > 1  # Remove single characters\n",
    "    ]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "# Apply stopword removal to the TOKENS column\n",
    "df_clean['TOKENS_FILTERED'] = df_clean['TOKENS'].apply(remove_stopwords)\n",
    "\n",
    "print(\"✓ Stopword removal completed. 'TOKENS_FILTERED' column created.\")\n",
    "print(f\"New DataFrame shape: {df_clean.shape}\")\n",
    "\n",
    "# Display verification of the filtered tokens for the first product\n",
    "print(\"\\nVerification of Stopword Removal (First Product):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Original Tokens (Snippet): {df_clean['TOKENS'].iloc[0][:15]}\")\n",
    "print(f\"Filtered Tokens (Snippet): {df_clean['TOKENS_FILTERED'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc459ce9",
   "metadata": {},
   "source": [
    "## E. Lemmatization\n",
    "Lemmatization is the process of reducing different inflected forms of a word to a single base form, known as the lemma. This is more sophisticated than stemming, as it relies on a dictionary or vocabulary to ensure the root form is an actual word (e.g., changing \"crocheting\" to \"crochet,\" or \"leaves\" to \"leaf\").\n",
    "\n",
    "This step ensures that variations of the same product attribute or material are treated as one feature by the classification model, reducing the total vocabulary size and improving prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c5793ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Lemmatization completed. 'TOKENS_LEMMATIZED' column created.\n",
      "New DataFrame shape: (618, 12)\n",
      "\n",
      "Verification of Lemmatization (First Product):\n",
      "--------------------------------------------------\n",
      "Filtered Tokens (Snippet): ['Flowers', 'Convertible', 'Puso', 'Wedding', 'Tote', 'hobo-style', 'embroidered', 'floral', 'motifs', 'weddings', 'crafted', 'upcycled', 'fabric', 'leather', 'White', 'UNSPECIFIED', 'Upcycled', 'Fabric', 'Leather']\n",
      "Lemmatized Tokens (Snippet): ['Flowers', 'Convertible', 'Puso', 'Wedding', 'Tote', 'hobo-style', 'embroidered', 'floral', 'motif', 'wedding', 'crafted', 'upcycled', 'fabric', 'leather', 'White', 'UNSPECIFIED', 'Upcycled', 'Fabric', 'Leather']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# --- Lemmatization Setup ---\n",
    "# Note: Ensure wordnet and omw-1.4 were downloaded in an earlier step\n",
    "# nltk.download('wordnet', quiet=True)\n",
    "# nltk.download('omw-1.4', quiet=True) \n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"Reduces tokens to their base/root form (lemma).\"\"\"\n",
    "    if not tokens:\n",
    "        return []\n",
    "    \n",
    "    # Apply lemmatization to each token in the list\n",
    "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return lemmatized\n",
    "\n",
    "# Apply lemmatization to the filtered tokens column\n",
    "df_clean['TOKENS_LEMMATIZED'] = df_clean['TOKENS_FILTERED'].apply(lemmatize_tokens)\n",
    "\n",
    "print(\"✓ Lemmatization completed. 'TOKENS_LEMMATIZED' column created.\")\n",
    "print(f\"New DataFrame shape: {df_clean.shape}\")\n",
    "\n",
    "# Display verification of the lemmatized tokens for the first product\n",
    "print(\"\\nVerification of Lemmatization (First Product):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Filtered Tokens (Snippet): {df_clean['TOKENS_FILTERED'].iloc[0]}\")\n",
    "print(f\"Lemmatized Tokens (Snippet): {df_clean['TOKENS_LEMMATIZED'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b32512",
   "metadata": {},
   "source": [
    "## F. Target Label Encoding\n",
    "The target variable of model is the Tags column, which contains a string of comma-separated tags (e.g., \"wedding, tote, floral embroidery\").\n",
    "\n",
    "Multi-label encoding converts this string into a binary matrix (or multi-hot encoded vector) .\n",
    "- Each unique tag in the entire dataset becomes a separate column.\n",
    "- For each product, a 1 is placed in the column corresponding to a tag that applies to that product, and a 0 is placed everywhere else.\n",
    "\n",
    "This process is necessary because the Multi-Label Classifier predicts a probability for every single possible tag simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56fec333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Target Labels encoded into binary matrix (Y).\n",
      "Total Unique Tags Discovered: 717\n",
      "Target Matrix Shape (Rows, Tags): (618, 717)\n",
      "\n",
      "Verification of Target Label Encoding (First 5 Rows):\n",
      "----------------------------------------------------------------------\n",
      "                                                      Tags  1940s movie  1980s movie  1990s movie  2 custom heart Instagram decals  Acrylic Elysse parol ornament\n",
      "0  wedding, tote, floral embroidery, Filipino, sustainable            0            0            0                                0                              0\n",
      "1                  jeepney, handbag, Filipino, sustainable            0            0            0                                0                              0\n",
      "2                        fanny pack, Filipino, sustainable            0            0            0                                0                              0\n",
      "3              sling bag, Filipino, handwoven, sustainable            0            0            0                                0                              0\n",
      "4         shoulder bag, woven waves, Filipino, sustainable            0            0            0                                0                              0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# --- Data Preparation for Encoding ---\n",
    "\n",
    "# 1. Clean up and split the 'Tags' column\n",
    "# The 'Tags' column contains comma-separated strings (e.g., 'tag1, tag2, tag3').\n",
    "# We must split this string into a list of individual tags.\n",
    "\n",
    "# Assuming df_clean is your current DataFrame with the 'Tags' column\n",
    "# Apply a lambda function to split the string by comma and remove surrounding whitespace\n",
    "df_clean['TAGS_LIST'] = df_clean['Tags'].apply(\n",
    "    lambda x: [tag.strip() for tag in x.split(',')] if isinstance(x, str) and x.strip() else []\n",
    ")\n",
    "\n",
    "# 2. Initialize and Fit the MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Fit the binarizer to all the tags across the entire dataset\n",
    "# This discovers all unique tags and assigns them an index\n",
    "Y_labels = mlb.fit_transform(df_clean['TAGS_LIST'])\n",
    "\n",
    "# 3. Create the final Target DataFrame (Y)\n",
    "# Convert the binary matrix back into a labeled DataFrame\n",
    "Y = pd.DataFrame(Y_labels, columns=mlb.classes_)\n",
    "\n",
    "# 4. Concatenate Y back to the main DataFrame (optional, but good for inspection)\n",
    "df_encoded = pd.concat([df_clean.reset_index(drop=True), Y], axis=1)\n",
    "\n",
    "print(\"✓ Target Labels encoded into binary matrix (Y).\")\n",
    "print(f\"Total Unique Tags Discovered: {len(mlb.classes_)}\")\n",
    "print(f\"Target Matrix Shape (Rows, Tags): {Y.shape}\")\n",
    "\n",
    "# Display verification of the target encoding\n",
    "print(\"\\nVerification of Target Label Encoding (First 5 Rows):\")\n",
    "print(\"-\" * 70)\n",
    "# Show the original tags and the first few encoded tag columns\n",
    "print(df_encoded[['Tags'] + list(Y.columns)[:5]].head(5).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadbf441",
   "metadata": {},
   "source": [
    "## G. Targeted Pruning\n",
    "Identify and remove non-semantic noise tags (like branded or ultra-specific single-count tags) from the $\\mathbf{Y}$ matrix. Because noise tags are mixed with meaningful rare tags, a clearly defined set of noise criteria is required to preserve only high-value descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8441b1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of tags identified as noise based on low semantic value or duplication (Count 1-4)\n",
    "noise_tags_to_drop = [\n",
    "    # Too specific/non-generalizable\n",
    "    '2 custom heart Instagram decals', 'Acrylic Elysse parol ornament', 'Acrylic Sampabell parol ornament', \n",
    "    'Acrylic Tala parol ornament', 'Acrylic parol ornament special with ring', \n",
    "    'Acrylic parol ornament special with ring duplicate', 'Acrylic tulip parol ornament', \n",
    "    'Wooden Capiz parol ornament variant 2', 'Wooden Capiz parol ornament variant', \n",
    "    'Y2K Heart Tsurikawa', 'Wooden tricycle ornament', 'Wooden pride fist ornament', \n",
    "    'White enamel Capiz parol ornament', 'White Men Can\\'t Jump', 'Wedgwood', 'Wheel acrylic keychain', \n",
    "    'Vincent Van Beau', 'Utot shirt', 'Utot', 'Tsinelas keychain', 'Tres Santan motif', \n",
    "    'Tree blueprint notepad', 'Time of Your Life', 'Tilso Japan', 'Thomas', 'Theology books sticker pack', \n",
    "    'Theology books keychain', 'Teletubbies', 'Tboli malong', 'Tboli', 'Tagalog humor', \n",
    "    'Tagalog design', 'Tagalog hat', 'Tagalog ay nako sticker', 'Tabo keychain', 'Sungka keychain', \n",
    "    'Sun Araw stud', 'Sun Araw statement necklace', 'Sun Araw statement hoops', 'Sun Araw silver necklace',\n",
    "    'Sun Araw silver hoops', 'Sun Araw pearl ring', 'Sun Araw hoop earrings', 'Sun Araw enamel pin',\n",
    "    'Sun Araw + pearl dangle', 'Sun + pearl studs silver', 'Sun + pearl dangle gold', \n",
    "    'Rice knuckle cooking printable', 'Philippine Sun friendship bracelet', \n",
    "    'Philippine pearls bridal trio', 'Philippine pearls bridal drop necklace', \n",
    "    'Philippine parol', 'Philippine flag heart pin', 'Philippine Sun hoop earrings silver', \n",
    "    'Philippine Sun pearl dangle silver', 'Philippine Sun pearl necklace silver', \n",
    "    'Philippine farming regions print', 'Philippine flag graduation stole variant', \n",
    "    'Philippine flag graduation stole limited', 'Philippine flag graduation stole', \n",
    "    'Philippine flag', 'Philippine Sun bracelet', 'Philippine Sun citrine necklace', \n",
    "    'Philippine Sun dainty earrings silver', 'Philippine Sun dangle earrings silver', \n",
    "    'Philippine Sun enamel pin', 'Philippine Sun hoop earrings gold', \n",
    "    'Philippine Sun necklace gold', 'Philippine Sun pearl bracelet silver', \n",
    "    'Pekpek Turbo sticker', 'Pearl scoop necklace PR-4', 'Pearl beaded Christmas ornament', \n",
    "    'toy', 'yoga mat', 'zip lips', 'Bahala ka sticker', 'Bahala Ka Sa Buhay Mo greeting card', \n",
    "    'Bad Dog Club', 'Babae print', 'Artsy floral bookmark', 'Artist sticker pack', \n",
    "    'Araw necklace', 'wedding heritage motif', 'woven waves', 'wrap top', 'wooden plaque', \n",
    "    'wooden keychain', 'whale sticker', 'wedding veil motif', 'wedding symbol motif', \n",
    "    'wedding symbol', 'Berenstain', 'Batman Forever', 'Batik button pins', 'Pekpek power print', \n",
    "    'Palau', 'Outline Sun Araw', 'Olo motif', 'NeverEnding Story', 'Nonom motif', \n",
    "    'Objects in mirror are cuter decal', 'Obsidian Sun Araw', 'Michael', 'Mikasa', \n",
    "    'Mini Capiz star ornaments', 'Motorcycle plate frame', 'Munggo food art printable', \n",
    "    'Jollibee reusable tote bag', 'John Travolta', 'Japanese sticker', 'Japanese illustration', \n",
    "    'James Cagney', 'Jack Russell', 'Jade Sun Araw', 'JDM wheel shoe charm', 'Itneg tapis', \n",
    "    'Italian cookbook', 'Indiana Glass', 'Ilocos Abra', 'Honda Civic Type R art print', \n",
    "    'Hellacute windshield banner sticker', 'Hellacute windshield banner', 'Hellacute tactical keychain',\n",
    "    'Hellacute heart croc charms', 'Hellacute croc charms', 'Healing vibes vapor rub printable', \n",
    "    'Haw Flakes candy printable', 'Halo-halo Filipino dessert sticker', 'Half Sun', \n",
    "    'Greenhouse house keychain', 'Green Pastures notepad', 'Godfather Part 1', 'Gengar shoe charms', \n",
    "    'Fried Green Tomatoes', 'Flag-inspired earrings', 'Free all political prisoners print', \n",
    "    'FRS/S2000/Miata/Type R charms', 'floral drop earrings', 'floral earrings', \n",
    "    'floral bloom earrings', 'flag pin', 'flag enamel pin', 'flag design', 'flag', \n",
    "    'fishtail motif', 'Filipino tarot', 'Filipino pet accessory', 'Filipino motif', \n",
    "    'Filipina nurse sticker set', 'Filipina mug', 'Filipina empowerment', 'Federal Windsor', \n",
    "    'Federal Pressed Glass', 'Eiffel Tower', 'Elizabeth Arden', 'Empowerment pin', \n",
    "    'Enamel Capiz parol ornament', 'Driving Miss Daisy', 'Diana Princess of Wales', \n",
    "    'Death to imperialism print', 'Dainty Sun Araw silver', 'Dainty Sun Araw dangle', \n",
    "    'Custom heart Instagram decal', 'Custom cherry blossom Instagram decal', \n",
    "    'Cristal D\\'Arques', 'Colonizers burned our fields print', 'Coconut grater sticker', \n",
    "    'Christian sticker pack', 'Christmas lantern stickers', 'Christmas décor', 'Cherry blossom valve stem caps', \n",
    "    'Cherry blossom Instagram decal variant 3', 'Cherry blossom Instagram decal variant 2', \n",
    "    'Cherry blossom Instagram decal variant', 'Cherry blossom Instagram decal', \n",
    "    'Cherry Blossom motorcycle frame', 'Capiz star ornaments', 'Capiz shell', \n",
    "    'Capiz mango tray set', 'Capiz flower napkin holders', 'Capiz candy cane ornaments', \n",
    "    'Burwood', 'Buri reindeer ornaments', 'unity cord motif', 'two-way earrings', 'turtle', \n",
    "    'tumbler', 'tulip candle holder', 'trinket box', 'tray', 'tradition motif', 'wedding pillow motif',\n",
    "    'Amethyst Sun Araw', 'Anime girl keychain', '1940s movie', '2 custom heart Instagram decals', \n",
    "    'Acrylic Elysse parol ornament', 'Acrylic Sampabell parol ornament', 'Acrylic Tala parol ornament', \n",
    "    'Acrylic parol ornament special with ring', 'Acrylic parol ornament special with ring duplicate', \n",
    "    'Acrylic tulip parol ornament',\n",
    "    # Specific Brand Names (Low-Value)\n",
    "    'Judy Belle', 'Royal Cornwall', 'Heisey', 'Lenox', 'Mikasa', 'Paul Revere', 'Thomas', 'Roger Duvoisin', \n",
    "    'Berman & Anderson', 'Anchor Hocking', 'Burwood', 'Hofbauer Byrdes', 'Indiana Glass', 'Cristal D\\'Arques', 'Federal Pressed Glass', 'Federal Windsor', 'Wedgwood',\n",
    "    # Product Metadata / Duplicates\n",
    "    'Unspecified', 'Parol', 'Parol dangle variant', 'Parol earrings', 'Parol earrings Christmas', \n",
    "    'Christmas lantern stickers', 'Christmas décor', 'Christmas lantern stickers', 'Parol coconut shell keychain',\n",
    "    'wall art', 'wall pocket', 'tote bag', 'serving tray', 'silver-plated', 'snowman', 'reversible', 'ribbed glass',\n",
    "    'rosary motif', 'rose pattern', 'rose pearl', 'rose pearl motif', 'rosette', 'rosette motif', 'sampaguita',\n",
    "    'sampaguita motif', 'santan flower', 'scrunchie', 'portrait sticker', 'pressed cut', 'pride fist necklace',\n",
    "    'printable poster', 'programming', 'puso', 'pyramid', 'raffia', 'rainbow', 'relish dish', 'reproduction bowl',\n",
    "    'resin', 'reverse psychology', 'pedicab', 'pendant', 'pestle', 'photo album', 'picnic mat', 'pillow', \n",
    "    'pineapple style', 'pitcher', 'platter', 'porcelain floral earrings', 'porcelain plaque', 'porcelain set',\n",
    "    'portrait illustration', 'portrait mug', 'owl', 'oval dish', 'ornament', 'nostalgia', 'net bag', 'mythology',\n",
    "    'music box', 'mother & child', 'mortar', 'monogram', 'minimalist motif', 'miniature jeepney', 'mini hoops',\n",
    "    'mahal kita print', 'mahal kita necklace', 'mahal kita', 'lingling-o earrings', 'lady frame', 'lamp',\n",
    "    'leaf motif', 'lighthouse', 'kutsinta', 'jewelry box', 'jacquard', 'inabel', 'hoop motif', 'hoodie',\n",
    "    'headscarf', 'hair clip', 'handbag', 'handloomed', 'handmade earrings', 'handmade keychain', \n",
    "    'handwoven skirt', 'handwoven table runner', 'halo-halo', 'gold plated hoops', 'glitter', 'fruit pattern',\n",
    "    'frog tote bag', 'fried chicken', 'floral vase', 'fishtail motif', 'flag and sun', 'flag pin', 'flag enamel pin',\n",
    "    'flag design', 'flag', 'floral bloom earrings', 'floral drop earrings', 'floral earrings', 'formal wear',\n",
    "    'gravy bowl', 'golden bloom motif', 'crystal dish', 'crystal bowl', 'fanny pack', 'fan design', \n",
    "    'family motif', 'faith motif', 'etched glass', 'embroidered', 'elephant', 'dominoes', 'distressed cap', \n",
    "    'dinuguan', 'dinner plate', 'different', 'decor', 'decanter', 'dad cap', 'cut glass', 'culture motif', \n",
    "    'creamer cup', 'cracker nut', 'coding power', 'coin wallet', 'coin motif', 'coffee mug', 'collectible plush',\n",
    "    'collectible mug', 'collectible frame', 'dog pin', 'dog lover', 'dog bandana', 'clutch bag', 'clutch', \n",
    "    'classic motif', 'clam shell', 'creative mind', 'candy bowl', 'calendar', 'collectible dish', 'capiz motif',\n",
    "    'carabao', 'cardinal bird', 'cat figurine', 'ceremony motif', 'champorado tuyo', 'charm', 'chicken inasal',\n",
    "    'bowl set', 'bowl', 'board game', 'bloom earrings', 'birds of paradise motif', 'bilo-bilo', 'bell', \n",
    "    'bees buddies', 'beer mug', 'beer', 'beaded keychain', 'bayong bag', 'bangle', 'balikbayan', 'bag', \n",
    "    'badge reel', 'backpack', 'ashtray', 'artist', 'arroz caldo', 'arrow design', 'arras motif', 'apparel', \n",
    "    'angel', 'abaca fiber', 'abaca bag', 'bridal necklace', 'bridal earrings', 'brass earrings', \n",
    "    'butterfly basket', 'butter dish', 'bubble glass', 'bridal set', 'caldereta', 'capiz shell dinnerware set', \n",
    "    'capiz shell', 'capiz', 'capiz christmas tree ornaments', 'Capiz', 'jollibee reusable tote bag', \n",
    "    'kamagong wood cross necklace', 'kapwa tarot', 'katol keychain', 'kawaii cow croc charms', \n",
    "    'kirby shoe charms set', 'klifus motif', 'kumain ka na ba sticker', 'longganisa', 'love motif', \n",
    "    'malong motif', 'mama bear', 'micro bag', 'kalesa', 'seashell', 'seaglass', 'seaglass earring', \n",
    "    'seaglass jewelry', 'seaglass necklace', 'seaglass pendant', 'seashell and pearl', 'seashell jewelry', \n",
    "    'sinigang ingredients print', 'sinigang mini print', 'star and sun araw dangle', 'stars and sun ear cuff', \n",
    "    'sun araw + pearl dangle', 'sun araw enamel pin', 'sun araw hoop earrings', 'sun araw pearl ring', \n",
    "    'sun araw silver hoops', 'sun araw silver necklace', 'sun araw statement hoops', 'sun araw statement necklace',\n",
    "    'sun araw stud', 'sunday morning', 'sungka keychain', 'tabo keychain', 'tagalog ay nako sticker', \n",
    "    'tagalog hat', 'tagalog humor', 'tboli', 'tboli malong', 'teletubbies', 'theology books keychain', \n",
    "    'theology books sticker pack', 'thomas', 'tilso japan', 'time of your life', 'tita tagalog necklace', \n",
    "    'to love is to resist print', 'todd parr', 'wooden pride fist ornament', 'wooden tricycle ornament', \n",
    "    'y2k heart tsurikawa', 'tree blueprint notepad', 'tres santan motif', 'tsinelas keychain', 'utot', 'utot shirt',\n",
    "    'vincent van beau', 'wedgwood', 'wheel acrylic keychain', 'white men can\\'t jump', 'white enamel capiz parol ornament',\n",
    "    'wooden \\'mahal kita\\' ornament', 'wooden capiz parol ornament', 'wooden capiz parol ornament duplicate', \n",
    "    'wooden sampabell parol ornament', 'wooden sampabell parol ornament variant', 'wooden bahay kubo ornament', \n",
    "    'wooden bahay kubo ornament duplicate', 'wooden basketball ornament', 'wooden firework parol ornament', \n",
    "    'wooden firework parol ornament variant', 'wooden jeepney ornament', 'wooden jeepney ornament variant', \n",
    "    'wooden jeepney ornament variant 2', 'wooden lechon ornament', 'wooden nurse heart + flag ornament', \n",
    "    'abaniko motif', 'acrylic box', 'adjustable', 'neverending story', 'nonom motif', \n",
    "    'objects in mirror are cuter decal', 'obsidian sun araw', 'john travolta', 'jollibee reusable tote bag', \n",
    "    'kamagong wood cross necklace', 'kapwa tarot', 'katol keychain', 'kawaii cow croc charms', \n",
    "    'kirby shoe charms set', 'klifus motif', 'kumain ka na ba sticker', 'birds of paradise motif', \n",
    "    'bloom earrings', 'board game', 'boho tote', 'arras motif', 'arrow design', 'arroz caldo', 'artist', \n",
    "    'ashtray', 'backpack', 'badge reel', 'bag', 'balikbayan', 'bangle', 'abaca fiber', 'abaca bag', \n",
    "    'brass earrings', 'bridal earrings', 'bridal necklace', 'bridal set', 'bubble glass', 'butter dish', \n",
    "    'butterfly basket', 'caldereta', 'bayong bag', 'beaded keychain', 'beer', 'beer mug', 'bees buddies', \n",
    "    'bell', 'bilo-bilo', 'birds of paradise', 'honda civic type r art print', 'ilocos abra', 'indiana glass',\n",
    "    'italian cookbook', 'itneg tapis', 'jdm wheel shoe charm', 'jack russell', 'jade sun araw', \n",
    "    'james cagney', 'japan pottery', 'ceremony motif', 'champorado tuyo', 'charm', 'chicken inasal', 'bowl', \n",
    "    'bowl set', 'coding power', 'coffee mug', 'coin motif', 'coin wallet', 'collectible dish', 'calendar', \n",
    "    'candy bowl', 'capiz motif', 'carabao', 'cardinal bird', 'cat figurine', 'lazy lechon sticker', 'lenox',\n",
    "    'leonard', 'lewd anime keychain', 'hofbauer byrdes', 'green pastures notepad', 'greenhouse house keychain',\n",
    "    'half sun', 'halo-halo filipino dessert sticker', 'haw flakes candy printable', \n",
    "    'healing vibes vapor rub printable', 'hellacute croc charms', 'hellacute heart croc charms', \n",
    "    'hellacute lanyard keychain', 'cracker nut', 'creamer cup', 'creative mind', 'clam shell', \n",
    "    'classic motif', 'clutch', 'dog bandana', 'dog lover', 'dog pin', 'collectible frame', \n",
    "    'collectible mug', 'collectible plush', 'compote bowl', 'condensed milk', 'condensed milk can', \n",
    "    'cord motif', 'japanese illustration', 'japanese sticker', 'jeepney charm', 'jo ann shirley', \n",
    "    'gengar shoe charms', 'godfather part 1', 'culture motif', 'cut glass', 'dad cap', 'decanter', \n",
    "    'decor', 'different', 'dinner plate', 'dinuguan', 'distressed cap', 'hellacute tactical keychain', \n",
    "    'hellacute windshield banner', 'hellacute windshield banner sticker', 'filipino motif', \n",
    "    'filipino pet accessory', 'filipino tarot', 'filipino traditional', 'filipino dad shirt', \n",
    "    'filipino decor', 'filipino dessert charm', 'floral vase', 'formal wear', 'dominoes', 'elephant',\n",
    "    'embroidered', 'enamel brass', 'etched glass', 'faith motif', 'family motif', 'fan design', \n",
    "    'fanny pack', 'crystal bowl', 'crystal dish', 'golden bloom motif', 'gravy bowl', \n",
    "    'flag-inspired earrings', 'free all political prisoners print', 'fried green tomatoes', \n",
    "    'genesis notepad', 'filipino bread', 'filipino car magnet', 'fried chicken', 'frog tote bag', \n",
    "    'fruit pattern', 'glitter', 'glossy pearl motif', 'filipino hat', 'filipino humor shirt', \n",
    "    'english', 'frs/s2000/miata/type r charms', 'federal pressed glass', 'federal windsor', \n",
    "    'filipina empowerment', 'filipina mug', 'filipina nurse sticker set', 'filipino christmas', \n",
    "    'kawaii', 'hair clip', 'halo-halo', 'handbag', 'handloomed', 'handmade earrings', \n",
    "    'handmade keychain', 'handwoven skirt', 'handwoven table runner', 'longganisa', 'love motif', \n",
    "    'malong motif', 'mama bear', 'micro bag', 'headscarf', 'hoodie', 'hoop motif', 'inabel', \n",
    "    'jacquard', 'jewelry box', 'filipino apparel', 'coconut grater sticker', \n",
    "    'colonizers burned our fields print', 'cristal d\\'arques', 'custom blossom instagram decal',\n",
    "    'miniature jeepney', 'minimalist motif', 'monogram', 'mortar', 'mother & child', 'music box', \n",
    "    'mythology', 'net bag', 'nostalgia', 'ornament', 'oval dish', 'kutsinta', 'lady frame', \n",
    "    'lamp', 'leaf motif', 'lighthouse', 'capiz flower napkin holders', 'capiz mango tray set', \n",
    "    'capiz shell', 'capiz shell dinnerware set', 'capiz star ornaments', \n",
    "    'cherry blossom motorcycle frame', 'cherry blossom instagram decal', \n",
    "    'cherry blossom instagram decal variant', 'cherry blossom instagram decal variant 2', \n",
    "    'cherry blossom instagram decal variant 3', 'cherry blossom valve stem caps', \n",
    "    'pearl dangle', 'pearl jewelry set', 'pearl necklace', 'pearls', 'mini hoops', \n",
    "    'porcelain plaque', 'porcelain set', 'portrait illustration', 'portrait mug', 'owl', \n",
    "    'palm', 'pandesal', 'paradise motif', 'patadyong', 'pear pin', 'pear pin set', 'eiffel tower',\n",
    "    'elizabeth arden', 'empowerment pin', 'enamel capiz parol ornament', \n",
    "    'capiz candy cane ornaments', 'rattan', 'religious', 'relish dish', 'reproduction bowl', \n",
    "    'resin', 'reverse psychology', 'pedicab', 'pendant', 'pestle', 'photo album', 'picnic mat', \n",
    "    'pillow', 'pineapple style', 'pitcher', 'platter', 'porcelain floral earrings', \n",
    "    'rose pearl motif', 'rosette', 'rosette motif', 'sampaguita', 'sampaguita motif', \n",
    "    'santan flower', 'scrunchie', 'portrait sticker', 'pressed cut', 'pride fist necklace', \n",
    "    'printable poster', 'programming', 'puso', 'pyramid', 'raffia', 'rainbow', 'bookstore notepad',\n",
    "    'bridal pearl cluster', 'bridal pearl set', 'bridal scoop pearl', 'bukaka', 'bukaka shirt', \n",
    "    'bukaka toddler tee', 'silver-plated', 'snowman', 'reversible', 'ribbed glass', \n",
    "    'ring pillow motif', 'rituals', 'rosary motif', 'rose pattern', 'rose pearl', \n",
    "    'serving tray', 'shell', 'shell keychain', 'shirt', 'silk', 'silk accent', 'silk flower', \n",
    "    'silk organza', 'christian sticker pack', 'christmas décor', 'christmas lantern stickers', \n",
    "    'coconut grater printable', 'berman & anderson', 'binakol', 'binakol jacket', \n",
    "    'bluso bnetek motif', 'teak vase', 'textile', 'threader earrings', 'toothpick holder', \n",
    "    'tote bag', 'spaghetti', 'sphere motif', 'spiral motif', 'spoon', 'star', 'sticker sheet', \n",
    "    'stocking', 'stud earrings', 'sun rays', 'sungka', 'sunset', 'vintage decor', 'vinyl', \n",
    "    'votive candle holder', 'wall art', 'wall pocket', 'waterproof', 'unity cord symbol', \n",
    "    'unity heritage motif', 'unity symbol', 'unity symbol motif', 'swan vase', 'sweatshirt', \n",
    "    'tapis wrap', 'tapsilog', 'tassel design', 'tassel earrings', 'wedding heritage motif', \n",
    "    'araw necklace', 'artist sticker pack', 'artsy floral bookmark', 'asian snacks', 'babae print',\n",
    "    'bad dog club', 'bahala ka sa buhay mo greeting card', 'bahala ka sticker', 'zip lips', \n",
    "    'bastos shirt', 'bastos toddler tee', 'yoga mat', 'yugal motif', 'anime girl car decal',\n",
    "    'toy', 'pearl beaded christmas ornament', 'pearl scoop necklace pr-4', 'pekpek turbo sticker', \n",
    "    'batik button pins', 'batman forever', 'berenstain', 'wedding symbol', 'wedding symbol motif', \n",
    "    'wedding veil motif', 'whale sticker', 'wooden keychain', 'wooden plaque', 'woven waves', 'wrap top',\n",
    "    'wedding bag', 'wedding gift'\n",
    "]\n",
    "\n",
    "# 1. Prune the Noise Tags from Y (Target Matrix)\n",
    "# Get all tag columns EXCEPT for the noise tags\n",
    "Y_pruned = Y.drop(columns=noise_tags_to_drop, errors='ignore')\n",
    "\n",
    "# 2. Prune the Noise Tags from the TAGS_LIST column (for Augmentation)\n",
    "def final_filter_tags(tag_list, tags_to_keep):\n",
    "    \"\"\"Filters a list of tags to only include those in the final model output.\"\"\"\n",
    "    return [tag for tag in tag_list if tag in tags_to_keep]\n",
    "\n",
    "tags_to_keep_final = Y_pruned.columns.tolist()\n",
    "\n",
    "df_clean['TAGS_LIST_FINAL'] = df_clean['TAGS_LIST'].apply(\n",
    "    lambda x: final_filter_tags(x, tags_to_keep_final)\n",
    ")\n",
    "\n",
    "print(\"✓ Noise tags successfully pruned from the target matrix (Y) and TAGS_LIST.\")\n",
    "print(f\"Original number of tags: {len(Y.columns)}\")\n",
    "print(f\"Final number of tags (after noise removal): {Y_pruned.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ce3867",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53ea7d59",
   "metadata": {},
   "source": [
    "## Conclusion of Data Engineering / Pre-processing\n",
    "The data preparation for the Multi-Label Classifier is now complete.\n",
    "- Input Feature (X): Created and cleaned (TEXT_CONTENT, tokenized, lemmatized).\n",
    "- Output Label (Y): Created and encoded into a binary matrix (Y DataFrame)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
